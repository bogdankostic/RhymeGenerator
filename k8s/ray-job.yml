# Job to run a Ray program in its own pod. Assumes that a cluster is already
# running (e.g., from './ray-cluster.yaml').
apiVersion: batch/v1
kind: Job
metadata:
  generateName: ray-rhyme-flant5-job-
spec:
  template:
    spec:
      restartPolicy: Never
      volumes:
        - name: ssh-volume
          configMap:
            name: ssh-config
        - name: rhyme-generator-pvc
          persistentVolumeClaim:
            claimName: rhyme-generator-pvc
        - name: publicfs
          flexVolume:
            driver: ceph.rook.io/rook
            fsType: ceph
            options:
              fsName: home
              clusterNamespace: rook-ceph
      containers:
        - name: ray-job
          image: bogdankostic/transformers-ray:latest
          imagePullPolicy: Always
          command: [ "/bin/bash", "-c", "--" ]
          args:
            - ray start --node-ip-address=$MY_POD_IP --num-cpus=0 --address=$RAY_HEAD_SERVICE_HOST:6379 --object-manager-port=12345 --node-manager-port=12346 --min-worker-port=11000 --max-worker-port=11003 &&
              cd /pvc/src &&
              python hpo_flan_t5.py
          volumeMounts:
            - name: publicfs
              mountPath: /pvc/publicfs
            - name: ssh-volume
              subPath: sshd_config
              mountPath: /etc/ssh/sshd_config
            - name: ssh-volume
              subPath: authorized_keys
              mountPath: /root/.ssh/authorized_keys
            - name: rhyme-generator-pvc
              mountPath: /pvc
          ports:
            - containerPort: 12345 # Ray internal communication.
            - containerPort: 12346 # Ray internal communication.
            - containerPort: 11000
            - containerPort: 11001
            - containerPort: 11002
            - containerPort: 11003
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
          resources:
            requests:
              cpu: 1000m
              memory: 4000Mi